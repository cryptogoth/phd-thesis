\documentclass{article}

\usepackage{amssymb}

\input{Qcircuit}

\begin{document}

\section{The Model 2D CCNTCM}

This is too powerful if we simply allow each module to be a single
qubit. Then it has arbitrary connectivity to every other qubit,
and we are back to CCAC. The requirement, however, that we have
the size of each module be $\Theta(n)$,
and moreover that $W = \Theta(n)\overline{W}$,
seems reasonable. Therefore, if any architecture tries to ``cheat'' by
having modules that are too small, and therefore the number of modules
be too numerous, we can propose a new parameter, which is the ratio
$W / \overline{W}$. We can also restrict the amount of teleportation
between modules, which is roughly the ratio $\overline{S} / \overline{W}$.

This model focuses on local nearest-neighbor constraints for physical
realism, and assumes in the large scale arbitrary connectivity again.

Things to mention about arbitrary connectivity: this does not really
cheat on depth, since we could implement entanglement between
all modules in a line, a larger-scale version of constant-depth
teleportation that we do on qubits within a module.

\section{Bounded Fan-in}

For quantum threshold gates, the usual construction in Hoyer-Spalek is
that we have a layer of $exact$ gates, from $exact(t)$ up to $exact(n)$
and take the parity of these in order to construct a
$threshold(t)$ gate. While this will work, it seems needlessly expensive
to me.

Within each exact gate, which uses the $\mu_{\phi_k}$ gate for
rotation by Hamming weight of the phase multiple $\phi_k = \frac{2\pi}{m}k$,
the input qubits are fanned out $n$ times, and for each one,
a $\mu_{\phi_k}$ gate is applied, for $k=1,\ldots, m$, where it is
sufficient for $m = n\log_2 n$. (Fact check this, whether it is
exact or not). Then there is a second layer which is just one
$\mu_{\phi_1}$ gate. I don't know how this works. This definitely needs
to be fact-checked since there is no dependence on the parameter $t$
anywhere. The good news is that the fan-in matters, which is the chief
advantage of the Yeh-Varvarigos construction over the Reif-Tate and
other papers. They bound their fan-in usually by a polynomial,
almost always $O(n)$. So the required precision of the denominator of
the
$R_z(\phi_k)$ rotation is bounded by $k = O(\log n)$ and not
$k = O(n)$ to give the usual $2^-k$ precision. This will come in handy
later when we have to compile to Clifford plus T gates, in that the
actual depth would be $O(\log \log n)$ instead of just $O(\log n)$,
or rather the square of that, for KSV.

The difference here is that $m = O(2^n)$, or exponential fan-in,
is the usual assumption, and leads to the normal $O(\log^2 n)$ depth
circuits for quantum compiling.

However, $\log \log (n)$ depth is still not constant, even though it
is very slow growing. This is too bad, but it leaves as a future open
question whether we can ever reduce these arithmetic functions to
constant-fanin circuits, or even logarithmic-fanin circuits.

Also, for $n = 4096$, taking the logarithmic base two twice and
squaring it is $144$, which is not so terrible a depth.

\section{The High-Level Sketch}

First we do multiple product of $n \times n$-bit quantum integers,
where each of the numbers is entangled with a control qubit from
phase estimation, and we can use the KSV numbers from before for
a more useful comparison with my polylog implementation. This uses
Theorem 5.4 from Yeh-Varvarigos.

Modular reduction follows Lemma 4.1 from Yeh-Varvarigos.
The $k$-bit representation of $a \bmod m$, where $a$ is an $n$-bit number
and $m$ is a $k$-bit number, can be computed from a majority circuit
with depth $D = O(1/\epsilon)$, size $S = O((1/\epsilon)kn^\epsilon)$, and
fan-in $O(n)$. For $\epsilon = 1$ and $k= n$, then we have depth $D=O(1)$,
size $S=O(n^3)$ and fan-in $F=O(n^2)$.

The two of these things together with measurement of the control qubits and
classical post-processing, is a complete factoring implementation.

\section{Quantum Compiling}

Given that quantum compiling, at least via the KSV method, or
the Cody Jones method, depends on quantum arithmetic, which is
based in large part on classical circuits. Before we were using
Toffoli's, the equivalent of AND-OR circuits, which is why we
were limited in depth. The only thing keeping us from diving
into the threshold circuit model completely is the lack of
unfanout.

That would be really soft failure, if I could finish Chapter 1
and then leave constant-depth unfanout for later.

\section{Majority Gates in 2D CCNTCM}

This is to continue mapping Andrea's gates, and check them against
the Hoyer-Spalek paper. The weights are unit, and we can consider
each majority gate as a module in itself. It has a linear number
of qubits, and then we don't have to worry about reordering qubits.
However, each majority gate only have one output, but this output
could be fanned out multiple times.

We don't really have a sense of the maximum number of gates in each
layer. For constant depth, all the size could be in each layer.
This is where the polynomial size bounds in submodules could
come in handy. For example, in multiplication, we use addition.
We know how many gates are needed to do partial product generation.

This maximum nubmer of gates in each layer would be useful to know
that we can contain a $O(n)$ amount of fan-out at the output of
every majority gate, to teleport to all the gates in the next layer
that depend on it for input.

\section{Fan-In}

We can upper bound the circuit width, which is not explicitly computed
for classical circuits, by the following formula

\begin{equation}
W \le F \cdot S / D
\end{equation}

Why is this valid? Due to the relationship between width and modules before
(having at least linear width) then we can assume we use fresh qubits
with every majority gate.

The upper-b

\end{document}