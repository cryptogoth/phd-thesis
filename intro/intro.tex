\chapter{Introduction}
\label{chap:intro}

One of the most remarkable scientific developments of the last twenty years is limited not just to one field of human knowledge, but it is exists at the confluence of many. Quantum computation, and quantum information, is the study of the power and ultimate limits of human abilities in the realm of computation and information processing. In particular, it applies to any machine that humans can build, given that all machines are physical objects and are therefore governed by the laws of physics. One definition of physics is the study of reality, or how to explain and predict the behavior of phenomenona in our universe. One definition of computer science is the study of problem-solving, or how information can be manipulated. From a human perspective, we can narrow such definitions: physics studies the phenomena which are observable to humans, at least in principle, given a long enough time and enough resources; computer science studies those problems which are solvable by humans and those problem-solving machines which are buildable and runnable by humans, again, given enough time and resources. This perspective also makes the union of physics and computer science (as well as mathematics, engineering, and other related fields) natural, although such a connection was not made until 1994.

The field of quantum computing effectively began that year when Peter Shor discovered that exploiting the mathematics of quantum physics, one could in theory build a quantum computer to determine the integer factors of a large number efficiently. This problem, known as factoring, resisted all attempts at solution from the time it was recognized by Karl Friedrich Gauss as the
problem ``to which we should bend our every effort'', the greatest problem of number theory, the subfield which he considered the jewel of mathematics, itself the queen of all the sciences.

Factoring, in addition to being a beautiful abstract problem, also has a very real implication for human affairs. Because of its difficulty on current digital computers, factoring has become the basis of the widely-used RSA cryptosystem, used to secure most of the worlds online transactions.

So now we have discovered that quantum computing is theoretically interesting because it can solve an important human problem. But can quantum computers be actually built?

Quantum architecture is the intermediate layer between algorithms and hardware.
It is the design of how quantum bits and their allowed interactions in order to solve these algorithms efficiently on realistic models of quantum hardware.
It aims to minimize circuit resources of interest, namely depth, size, and width. In analogy to classical circuits, the depth is the running time of an algorithm allowing parallelization, the size is the number of operations required over all parallel processors, and the width is the number of (quantum) bits required over all parallel processors.

In this dissertation, we study quantum architecture in the context of optimizing Shor's factoring algorithm on nearest-neighbor architectures with realistic constraints. It is hoped that lessons learned in this special-case can contribute to the general community general principles which can be used to generalize other quantum algorithms. We posit that the larger overall goal of quantum architecture should be the design of a general-purpose quantum processor, one which can execute any quantum algorithm with emphasis on being able to perform a core group of operations efficiently. This core group of operations is defined by the instruction set. Once quantum architecture has progressed to this point, we can leverage the remarkable strides in normal digital architecture over the past 80 years.

As well, we may be able to use the insights of quantum architecture to build quantum processors which can surpass digital architectures in the solution of large-scale problems over exponentially-sized solution spaces.

In Chapter \ref{chap:factor-polylog}, we present our first main result, a
nearest-neighbor architecture for factoring in polylogarithmic depth. This is an exponential improvement of the previous best-known result, which required quadratic depth.

In Chapter \ref{chap:factor-sublog}, we further improve our result with a
factoring architecture which executes in sublogarithmic depth, which represents another exponential improvement over our first result. Therefore, this achievement represents a remarkable doubly-exponential improvement over previous factoring architectures.

In Chapter \ref{chap:qcompile}, we study the relationship between quantum architecture and compiling. In digital computing, the boundary between architecture and compilers is quite porous and is determined by a processor's instruction set. Architecture studies processor resources to solve an algorithm given a particular instruction set which is fixed in hardware. This instruction set is produced by a compiler, a piece of (low-level) software which transforms over pieces of (high-level) software. This instruction set can change based on which algorithms it allows to solve efficiently as well as which processors it allows to manufacture efficiently as well as which operations it allows humans to understand easily. All of these factors combine to make architecture an art and an engineering discipline rather than merely a science.

Quantum computers make this problem even more difficult due to the nature of a quantum bit. Because transformations between quantum states vary continuously over the space of unitary matrices with complex coefficients, we can only approximate desired quantum logic gates using a fixed set, given to us by fault-tolerance.

This difficulty of approximation, which we call quantum compiling, represents the fundamental limit to the depth of factoring architectures. All known constant-depth factoring implementations, as well as sorting networks for those implementations which are not nearest-neighbor, assume arbitrary single-qubit rotations. That is, they assume the model where quantum compiling can be done in constant-depth, in polynomial size, for free. However, this is not the case when we combine these abstract circuit models with our model of fault-tolerant quantum computing. Under FTQC, quantum compiling depth represents a sufficient, but not known to be necessary, condition for constant-depth factoring. 

We leave this ultimately as an open problem and change directions.
Given that decreasing factoring depth with abandon increases size and width at relatively large polynomial growth rates (up to $n^8$). This is a rather undesirable time-space tradeoff.

Here, we insert the estimate of electricity and square miles and centuries of run-time based on our estimates given in the final exam.

In Chapter \ref{chap:coherence}, we introduce a novel quantum circuit resource called \emph{circuit coherence}.

We are now ready to state the thesis that we investigate in this dissertation.

\begin{quote}
Studying the depth of quantum architectures can help us design quantum computers that can solve
human problems with a human lifetime.
\end{quote}

Here, or somewhere else, we must distinguish between quantum architecture as a field and quantum architecture as a
circuit with hardware constraints. Actually, the two definitions are pretty close.

We also need to provide preliminary definitions of a quantum bit.

This deserves some unpacking. A quantum architecture is a circuit designed to implement a particular algorithm given certain
hardware constraints. The depth of a quantum architecture is the amount of time it takes for the circuit to complete.
Given a particular input $x$, how long (in human clock time) does it take to return the output of some function $f(x)$?
Quantum computers are interesting because for some algorithms, namely factoring, the depth of a quantum circuit is
less than the depth of any known classical circuit. However, this depth may correspond to a running time which is
still much longer than a single human lifetime. In particular this author, who is 32 years old at the time of this
writing, is interested in techniques to optimize the successful completion of a problem within his own lifetime.

Therefore, one meaning of ``studying the depth'' means to \emph{decrease} depth. The main technique for decreasing
depth is to parallelize a quantum circuit, that is, to divide and duplicate information to allow them to be processed
in a parallel way, using multiple classical controllers. However, parallelizing can increase other circuit resources,
usually due to this information duplication. This introduces a time-space tradeoff. In practical reality, we cannot
decrease depth at the expense of other circuit resources, such as the amount of information in a form suitable for
operation over time.

Decreasing depth can increase circuit size (the total number of operations to be performed) and circuit width
(the total amount of quantum information that can exist at any one time). This increase may again far
outstrip human resources, and delay completion of a quantum computer's construction, let alone its
successful execution, until well after that author's death. Therefore, the useful study of depth to help
quantum computers solve human problems must also limit this time-space tradeoff overhead. It should also
involve defining the exact tradeoff that is useful to minimize.

We have outlined the problems of designing such a low-depth quantum architecture so far in a very general,
abstract form. In this thesis, we will make these concepts more concrete by describing a formalism for
quantum circuits and quantum computation that is well-known to the research community.
Building on this formalism, we will propose a new architectural model and present our solutions to factoring in them.


Section \ref{sec:intro-arch} will discuss architectural models. We will present the definition of an
architectural models and give examples of them from the
current literature, with names like \textsf{AC} and \textsf{NTC}. We will discuss the idea of architectural
sub-models, with names like \textsf{2D CCNTC}. Finally,
we will describe our contribution of a new architectural model, called \textsf{2D CCNTCM}, which is a weakly 2D,
hybrid architecture that introduces the idea of allowing both short-range (local) interactions and long-range
interactions, and providing a configurable tradeoff in between them.

In order to build long-range interactions out of short-range interactions, Section \ref{sec:intro-cdc} 
gives a thorough background on recent constant-depth communication techniques. These include three
basic constructs for teleporting, copying, and uncopying quantum information across an unbounded
lattice.

All operations on a quantum architecture can be reduced to single-qubit operations and two-qubit operations.
The latter operations are what we have been calling ``interactions,'' and we have studied how to overcome
constraints on nearest-neighbor interactions using the constant-depth communication techniques previously.

Ultimately, however, the limits to depth of a quantum architecture are dependent on the single-qubit operations.

\section{Organization}