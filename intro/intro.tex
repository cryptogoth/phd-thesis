\chapter{Introduction}
\label{chap:intro}

% I. Quantum computing and perspectives
The combination of existing scientific disciplines provides fertile ground
for the development of entirely new scientific discoveries. Computer science
and quantum physics are now coming together in this way to give birth to
quantum computing. Computer science and quantum physics originated
as contemporaries in the early part of the 20th century, culminating in
technology that decisively ended World War II. They have remained
largely independent until recently, but the common thread connecting them
has always existed.

From the perspective of computer science, engineering is problem-driven.
We humans wish to solve a problem, like calculating ballistic weapon
trajectories, and we work backwards from there. To calculate,
we need to perform fast
arithmetic on stored numbers, for which we need to design devices in a
certain configuration, for which we need certain materials from nature.
At heart, it is the study of structured problem-solving which happens to have a strong
mechanical bias. This perspective asks the questions: what do we want to do,
and how can we
do it? It is like a child going into a LEGO store with a pre-existing plan
and only buying the bricks needed. It is an efficient plan for brick procurement,
but not
necessarily creative.

From the perspective of quantum physics, engineering is phenomena-based.
We humans wish to describe reality and observe nature. We discover objects
and effects in nature, and afterwards we may
then discover that they
can be useful in performing some useful task. An example is
the discovery of quantum tunnelling and its subsequent use to build a
field-effect transistor (FET) for digital electronics. This perspective asks the
questions: what is available to us, and what could we do it? It is like a child
going into a LEGO store with no pre-conceived plan, investigating all the
parts and dreaming up ways to use them. This will generate completely new
designs and models, but it is much slower than brick-shopping.

These perspectives are intertwined and serve complementary purposes,
and we often alternate
between the two of them. In our transistor example, we probably would not
have thought to build a FET if we had not already needed fast electrical
switching, for which vacuum tubes were proving inadequate. Moreover, we can
go beyond using quantum physics to simulate a classical device. There is
a remarkable example
of a completely new algorithm designed using the building blocks of
quantum physics. In 1994, Peter Shor contributed a quantum factoring algorithm
to solve a human problem efficiently,
one that has defied all classical algorithms before or since.
where no known classical algorithm, previously or since.
This kickstarted the entire field of quantum computing; united many
disciplines including computer science, physics, engineering, and
mathematics; and provided the foundation for this dissertation.

% II. Factoring
The problem of factorization, or factoring,
is simple to state and difficult to solve.
Given an integer $m$, return the integers $\{p_1, \ldots, p_k\}$ whose
product of their powers is $m$: $m = p_1^{\alpha_1}\cdots p_k^{\alpha_k}$.
It is a beautiful theoretical problem posed by Carl Friedrich Gauss in 1801,
one which he felt that every self-respecting scientist should devote their
efforts to solving. Factoring is an important problem in the theory of
numbers, which Gauss considered the crown jewel of mathematics, itself
considered the queen of the sciences.\footnote{\emph{Gauss zum Ged\"achtniss} (1856) by Wolfgang Sartorius von Waltershausen}
It is a quirk of cryptography that the hardness of problems can be used
to secure communication. Therefore, factoring also has immense practical
usefulness as the basis of the RSA cryptosystem, the most widely-used
cryptosystem around the world and on the internet.

The somewhat sinister,
but financially important, importance to
governments is the ability to gather intelligence on each other and on their
citizens. The importance to citizens is the ability to choose security
parameters so that their correspondence is protected for a limited time.
The organizers of the popular democratic uprisings in the Middle East known
as the Arab Spring in 2011 definitely benefitted from using a cryptosystem that was
secure against compromise given the resources of the regimes that they toppled.
A government able to hack the organizers' Twitter accounts or Facebook pages
could misdirect protest actions or identify activists for imprisonment.
Similarly, being able to accurately estimate the resources needed to factor
an RSA key of a certain size may help protect future citizens against
governments with a quantum computers, as governments will be the first
organizations interested in and able to afford such a device.

How can we reason about the computational resources needed to solve the
problem of factoring an $n$-bit number?
Before 1994, the best known algorithm was the
number field sieve (citation needed) which had a running time of
$exp(\sqrt[3]{log n})$. This can be considered a pre-quantum (classical)
algorithm in a pre-quantum era. Its sub-exponential (but still super-polynomial)
running time is considered intractable and empirical proof of its hardness.
This conjectured hardness is why users of the RSA cryptosystem above have faith
that their information is resistant to compromise, while at the same time
giving hope to the possibility of discovering some as yet unknown improvement.

In 1994, Peter Shor fulfilled this hope in a landmark
discovery that using the more general computational power of quantum physics,
factoring could be performed in running time $O(n^3\log n \log\log n)$ (citation
needed). 
The computational model in which this remarkable feat was achieved
made some assumptions about quantum computing devices, which are now
standard in the field. 
However, many remain skeptical of this achievement due to the perceived
plausibility of these assumptions, which is why RSA still remains the
most widely-used cryptosystem in the world.\footnote{Despite recommendations
from the NSA to switch to ECC, itself susceptible to quantum attacks (citation needed).}
These assumptions include that robust error-correction
can be performed to preserve fragile quantum information and that quantum gates
can be performed with high fidelity. Whether these assumptions can be
realized is an active area of experimental physics research, and is not the
topic of this current work. However, given that these assumptions will
eventually be realized, how can we optimize Shor's factoring algorithm?
Quantum computation is now a rapidly maturing field, and we cannot provide
an adequate background in this dissertation. The interested reader is referred
to the standard textbook in the field by Nielsen and Chuang \cite{Nielsen2000}.
Building upon this background, we can provide an introduction into the subfield
of quantum architecture.

% III. Quantum circuits and quantum architecture
First, we will introduce a quantum circuit.
Analogous to the classical circuit model, a quantum circuit consists of
a network, or graph, of gates operating on quantum bits (qubits). One can
arrange this graph such that directed edges represent qubits and
nodes represent reversible quantum gates with in-degree equal to out-degree.
See Figure \ref{fig:intro-qcirc} for a representation of a quantum circuit
as a special class of directed
acyclic graphs. All flows travel from the input state of the qubits at the
first timestep (on the far
left) to the final output state on the same qubits in the last timestep
(on the far right). This induces a natural definition of circuit depth,
size, and width, which we will make more formal later.

\begin{figure}
\caption{A quantum circuit as a directed acyclic graph.}
\label{fig:intro-qcirc}
\end{figure}

A quantum architecture is a quantum circuit with constraints. For this
dissertation, we will concern ourselves with two main constraints:
(1) the layout graph of qubits which constrain two-qubit gate interactions and
(2) the decomposition of all single-qubit gates to a discrete, finite,
set of gates which are universal combined with a fixed two-qubit gate.
Quantum architecture provides an intermediate layer in between working
with actual hardware (the physical technology in a laboratory) and the
abstract theory (algorithms divorced of all physical constraints).
As quantum architects, we can optimize algorithms in a general way which
is relevant to many kinds of laboratory experiments. At the same time,
we can take into account the more complicated nature of physical devices
to provide more pessimistic, accurate, and numerical resource estimates than more
high-level theoretical results.
The role of quantum architecture is two-fold. First,
to quantify and improve the resources---such as circuit depth,
size, and width mentioned above---required to run a quantum
algorithm on a system of varying degrees of realism. Second, to provide
tradeoffs between circuit resources, or conceptual knobs used to tune a
particular implementation. This allows experimentalists to configure a
quantum architecture to meet their needs in a laboratory while allowing
theorists to improve tradeoffs or design new algorithms which take
physical constraints into account.


% Point 
We are now ready to state the thesis that we investigate in this dissertation.

\begin{quote}
Studying the depth of quantum architectures can help us design quantum computers that can solve
human problems with a human lifetime.
\end{quote}

Here, or somewhere else, we must distinguish between quantum architecture as a field and quantum architecture as a
circuit with hardware constraints. Actually, the two definitions are pretty close.

We also need to provide preliminary definitions of a quantum bit.

This deserves some unpacking. A quantum architecture is a circuit designed to implement a particular algorithm given certain
hardware constraints. The depth of a quantum architecture is the amount of time it takes for the circuit to complete.
Given a particular input $x$, how long (in human clock time) does it take to return the output of some function $f(x)$?
Quantum computers are interesting because for some algorithms, namely factoring, the depth of a quantum circuit is
less than the depth of any known classical circuit. However, this depth may correspond to a running time which is
still much longer than a single human lifetime. In particular this author, who is 32 years old at the time of this
writing, is interested in techniques to optimize the successful completion of a problem within his own lifetime.

Therefore, one meaning of ``studying the depth'' means to \emph{decrease} depth. The main technique for decreasing
depth is to parallelize a quantum circuit, that is, to divide and duplicate information to allow them to be processed
in a parallel way, using multiple classical controllers. However, parallelizing can increase other circuit resources,
usually due to this information duplication. This introduces a time-space tradeoff. In practical reality, we cannot
decrease depth at the expense of other circuit resources, such as the amount of information in a form suitable for
operation over time.

Decreasing depth can increase circuit size (the total number of operations to be performed) and circuit width
(the total amount of quantum information that can exist at any one time). This increase may again far
outstrip human resources, and delay completion of a quantum computer's construction, let alone its
successful execution, until well after that author's death. Therefore, the useful study of depth to help
quantum computers solve human problems must also limit this time-space tradeoff overhead. It should also
involve defining the exact tradeoff that is useful to minimize.

We have outlined the problems of designing such a low-depth quantum architecture so far in a very general,
abstract form. In this thesis, we will make these concepts more concrete by describing a formalism for
quantum circuits and quantum computation that is well-known to the research community.
Building on this formalism, we will propose a new architectural model and present our solutions to factoring in them.


Section \ref{sec:intro-arch} will discuss architectural models. We will present the definition of an
architectural models and give examples of them from the
current literature, with names like \textsf{AC} and \textsf{NTC}. We will discuss the idea of architectural
sub-models, with names like \textsf{2D CCNTC}. Finally,
we will describe our contribution of a new architectural model, called \textsf{2D CCNTCM}, which is a weakly 2D,
hybrid architecture that introduces the idea of allowing both short-range (local) interactions and long-range
interactions, and providing a configurable tradeoff in between them.

In order to build long-range interactions out of short-range interactions, Section \ref{sec:intro-cdc} 
gives a thorough background on recent constant-depth communication techniques. These include three
basic constructs for teleporting, copying, and uncopying quantum information across an unbounded
lattice.

All operations on a quantum architecture can be reduced to single-qubit operations and two-qubit operations.
The latter operations are what we have been calling ``interactions,'' and we have studied how to overcome
constraints on nearest-neighbor interactions using the constant-depth communication techniques previously.

Ultimately, however, the limits to depth of a quantum architecture are
dependent on the single-qubit operations. 

The remainder of this chapter is devoted to the necessary background needed
to understand quantum architecture in general and factoring architectures
in particular. Quantum architecture has potentially many concerns, but we
focus on two of them: interaction distance and fault-tolerant quantum compiling.
Without loss of generality, the gates of a quantum circuit can be reduced to
single-qubit and two-qubit gates. In fact, we can use a fixed two-qubit gate
and combine it with arbitrary single-qubit gates.
Section \ref{sec:intro-basis} provides the necessary background for
understanding single-qubit gates, which do not impose any constraints on
which qubits can interact with each other.

, its relationship to quantum compiling,
recent constant-depth techniques in communicating quantum information across
a lattice, and most importantly,
the usefulness of architectural modules,
and the conc

\input{intro/intro-basis.tex}

\input{intro/intro-arch.tex}

\input{intro/intro-cdc.tex}

\input{intro/intro-modules.tex}

% Old leftover stuff
So now we have discovered that quantum computing is theoretically interesting because it can solve an important human problem. But can quantum computers be actually built?

Quantum architecture is the intermediate layer between algorithms and hardware.
It is the design of how quantum bits and their allowed interactions in order to solve these algorithms efficiently on realistic models of quantum hardware.
It aims to minimize circuit resources of interest, namely depth, size, and width. In analogy to classical circuits, the depth is the running time of an algorithm allowing parallelization, the size is the number of operations required over all parallel processors, and the width is the number of (quantum) bits required over all parallel processors.

In this dissertation, we study quantum architecture in the context of optimizing Shor's factoring algorithm on nearest-neighbor architectures with realistic constraints. It is hoped that lessons learned in this special-case can contribute to the general community general principles which can be used to generalize other quantum algorithms. We posit that the larger overall goal of quantum architecture should be the design of a general-purpose quantum processor, one which can execute any quantum algorithm with emphasis on being able to perform a core group of operations efficiently. This core group of operations is defined by the instruction set. Once quantum architecture has progressed to this point, we can leverage the remarkable strides in normal digital architecture over the past 80 years.

As well, we may be able to use the insights of quantum architecture to build quantum processors which can surpass digital architectures in the solution of large-scale problems over exponentially-sized solution spaces.

In Chapter \ref{chap:factor-polylog}, we present our first main result, a
nearest-neighbor architecture for factoring in polylogarithmic depth. This is an exponential improvement of the previous best-known result, which required quadratic depth.

In Chapter \ref{chap:factor-sublog}, we further improve our result with a
factoring architecture which executes in sublogarithmic depth, which represents another exponential improvement over our first result. Therefore, this achievement represents a remarkable doubly-exponential improvement over previous factoring architectures.

In Chapter \ref{chap:qcompile}, we study the relationship between quantum architecture and compiling. In digital computing, the boundary between architecture and compilers is quite porous and is determined by a processor's instruction set. Architecture studies processor resources to solve an algorithm given a particular instruction set which is fixed in hardware. This instruction set is produced by a compiler, a piece of (low-level) software which transforms over pieces of (high-level) software. This instruction set can change based on which algorithms it allows to solve efficiently as well as which processors it allows to manufacture efficiently as well as which operations it allows humans to understand easily. All of these factors combine to make architecture an art and an engineering discipline rather than merely a science.

Quantum computers make this problem even more difficult due to the nature of a quantum bit. Because transformations between quantum states vary continuously over the space of unitary matrices with complex coefficients, we can only approximate desired quantum logic gates using a fixed set, given to us by fault-tolerance.

This difficulty of approximation, which we call quantum compiling, represents the fundamental limit to the depth of factoring architectures. All known constant-depth factoring implementations, as well as sorting networks for those implementations which are not nearest-neighbor, assume arbitrary single-qubit rotations. That is, they assume the model where quantum compiling can be done in constant-depth, in polynomial size, for free. However, this is not the case when we combine these abstract circuit models with our model of fault-tolerant quantum computing. Under FTQC, quantum compiling depth represents a sufficient, but not known to be necessary, condition for constant-depth factoring. 

We leave this ultimately as an open problem and change directions.
Given that decreasing factoring depth with abandon increases size and width at relatively large polynomial growth rates (up to $n^8$). This is a rather undesirable time-space tradeoff.

Here, we insert the estimate of electricity and square miles and centuries of run-time based on our estimates given in the final exam.

In Chapter \ref{chap:coherence}, we introduce a novel quantum circuit resource called \emph{circuit coherence}.
